%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MCM/ICM LaTeX Template %%
%% 2026 MCM/ICM           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt]{article}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{array}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{newtxtext, newtxmath}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\Problem}{C}
\newcommand{\Team}{2614177}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Set page margins
\geometry{left=1in, right=1in, top=1in, bottom=1in}

% Header and footer setup
\pagestyle{fancy}
\lhead{Team \Team}
\chead{}
\rhead{\Problem}
\cfoot{Page \thepage\ of \pageref{LastPage}}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}

% Section formatting
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
	
	% Summary Sheet Page
	\thispagestyle{empty}
	\begin{center}
		\begin{tabular}{ccc}
			\parbox[t]{0.3\linewidth}{\centering\textbf{Problem Chosen}\\ \Large \textcolor{red}{\Problem}} &
			\parbox[t]{0.3\linewidth}{\centering\textbf{2026\\ MCM/ICM\\ Summary Sheet}} &
			\parbox[t]{0.3\linewidth}{\centering\textbf{Team Control Number}\\ \Large \textcolor{red}{\Team}} \\
			\hline
		\end{tabular}
	\end{center}
	
	\vspace{0.5cm}
	
	\begin{center}
		\textbf{\Large Dancing with the Data: A Dual-Engine Hybrid Approach for Vote Reconstruction and Mechanism Optimization}
	\end{center}
	
	\begin{abstract}
		In the context of \emph{Dancing with the Stars}, the opacity of fan voting data presents a significant challenge for analyzing competition fairness and optimizing program mechanisms. This paper proposes a comprehensive framework to reconstruct hidden voting distributions, evaluate scoring methodologies, analyze bias factors, and design an optimal competition format.
		
		For \textbf{Task 1}, we developed a \textbf{Hybrid MAP-MCMC Dual-Engine Model} to reconstruct the latent fan voting percentages. By integrating Maximum A Posteriori (MAP) estimation for coarse localization with Markov Chain Monte Carlo (MCMC) for fine-grained search, and further incorporating a temporal smoothing prior based on Machine Learning (ML), we successfully recovered the voting data for 335 historical weeks with an average accuracy of \textbf{98.21\%}.
		
		For \textbf{Task 2}, we conducted a \textbf{Counterfactual Simulation} to compare the standard "Ranking Method" with the raw "Percentage Method". We identified a \textbf{17.91\% Disagreement Rate} between the two methods. Our analysis confirms that the Ranking Method is superior in protecting "Judge Favorites" (96.9\% survival rate vs. 91.3\% under Percentage Method), acting as a necessary safety net against fan volatility.
		
		For \textbf{Task 3}, we employed a \textbf{Hybrid Attribution Model} (Linear OLS for judges + Non-linear Polynomial for fans) to quantify the impact of demographics. We discovered an \textbf{"Age Paradox"}: Judges penalize age linearly ($\beta = -0.038$), while fans exhibit a U-shaped preference. Additionally, judges show a bias towards Actors (+0.027), whereas fans significantly favor Athletes (+0.004).
		
		For \textbf{Task 4}, we designed a \textbf{Multi-Objective Mechanism Optimization} framework. Through fine-grained grid search, we identified the optimal strategy: \textbf{"Early Fan Empowerment, Late Judge Control"} (10\%/90\% split early $\to$ 45\%/55\% split late), combined with a \textbf{"Bottom 3 Judge Save"} mechanism. This configuration reduces the unfair elimination rate of technical talents to \textbf{$<$0.5\%} while maintaining high viewer engagement.
		
		\textbf{Keywords:} Inverse Problem, MCMC, Counterfactual Simulation, Mechanism Design, Multi-Objective Optimization
	\end{abstract}
	
	\newpage
	
	% Main content starts here
	\setcounter{page}{1}
	\pagestyle{fancy}
	\tableofcontents
	\newpage
	
	\section{Introduction}
	
	\subsection{Background and Motivation}
	Reality television has become a dominant cultural force in the 21st century, with \emph{Dancing with the Stars} (DWTS) standing as a prime example of the genre's longevity and global appeal. The show's core mechanic—pairing celebrities with professional dancers and subjecting them to a dual evaluation system involving expert judges and public voting—creates a compelling narrative of growth, artistry, and popularity. However, this dual-scoring system introduces a fundamental tension: the conflict between \textit{meritocracy} (technical skill as evaluated by professionals) and \textit{democracy} (popularity as expressed by the viewing public).
	
	While judge scores are transparent and publicly available, the distribution of audience votes is kept strictly confidential by the producers. This opacity creates a "black box" that obscures the true dynamics of the competition. Controversial eliminations, where high-scoring dancers are ousted in favor of popular but less skilled contestants, frequently spark public debate about the fairness of the system. This phenomenon raises critical questions: Is the current voting system optimally designed? Does it protect talent, or does it cater excessively to popularity?
	
	In this paper, we aim to demystify the voting process. By treating the unknown fan votes as latent variables in a complex system, we seek to reconstruct history, analyze the fairness of different scoring methods, and propose a mathematically optimized mechanism for future seasons.
	
	\subsection{Problem Restatement}
	The overarching goal of this study is to analyze the voting and scoring mechanisms of DWTS to ensure a balance between fairness (rewarding skill) and engagement (respecting viewer preference). Specifically, we are tasked with four distinct but interconnected problems:
	
	\begin{itemize}
		\item \textbf{Task 1: Reconstruction.} Develop a mathematical model to estimate the hidden percentage of fan votes ($V_F$) for every couple in every week of the show's history, using only the visible judge scores ($S_J$) and the final elimination results ($E$).
		\item \textbf{Task 2: Evaluation.} Using the reconstructed data, compare the current "Ranking Method" (where scores and votes are converted to ranks) against a hypothetical "Percentage Method" (where raw percentages are summed). Determine which method is "fairer" and under what conditions.
		\item \textbf{Task 3: Factor Analysis.} Investigate the influence of demographic and categorical factors—such as age, gender, and industry (e.g., actor, athlete, reality star)—on both judge scores and fan votes. Identify any systematic biases.
		\item \textbf{Task 4: Optimization.} Design a new, optimized scoring mechanism. This involves determining the ideal weight distribution between judges and fans ($w_J, w_F$) and potentially introducing new rules (e.g., a "Judge Save") to maximize a composite objective of fairness and viewer retention.
	\end{itemize}
	
	\subsection{General Assumptions}
	To make the modeling process tractable, we adopt the following assumptions:
	\begin{enumerate}
		\item \textbf{Rationality of Judges:} Judge scores are primarily a reflection of technical performance, though they may contain some subjective bias.
		\item \textbf{Consistency of Fan Preferences:} While fan votes fluctuate, a contestant's "base popularity" is relatively stable over short periods (e.g., consecutive weeks), allowing for temporal smoothing.
		\item \textbf{Closed System:} The total percentage of fan votes in any given week sums to 100\% (or 1.0).
		\item \textbf{Elimination Determinism:} The elimination result is a deterministic function of the combined score, except in cases of tie-breakers which follow known rules.
	\end{enumerate}
	
	\section{Task 1: Reconstructing the Black Box}
	
	\subsection{Problem Formulation: An Ill-Posed Inverse Problem}
	Let $N_t$ be the number of couples in week $t$.
	Let $\mathbf{S}_t \in \mathbb{R}^{N_t}$ be the vector of judge scores.
	Let $\mathbf{V}_t \in \mathbb{R}^{N_t}$ be the unknown vector of fan vote percentages, such that $\sum_{i=1}^{N_t} V_{t,i} = 1$ and $V_{t,i} > 0$.
	
	The competition rules state that the final combined score $C_{t,i}$ for couple $i$ is:
	\begin{equation}
		C_{t,i} = \text{Rank}(\mathbf{S}_t)_i + \text{Rank}(\mathbf{V}_t)_i
	\end{equation}
	where $\text{Rank}(\cdot)$ maps values to integers from 1 (lowest) to $N_t$ (highest). The couple with the lowest combined score $C_{t,i}$ is eliminated. Let $E_t$ be the set of eliminated couples observed in data.
	
	Our goal is to find $\mathbf{V}_t$ such that the implied elimination matches $E_t$. This is an \textbf{ill-posed inverse problem} because the mapping from $\mathbf{V}_t$ to $E_t$ is non-injective; infinitely many vote distributions can result in the same elimination outcome.
	
	\subsection{The Hybrid MAP-MCMC Dual-Engine Model}
	To solve this, we propose a two-stage approach: a coarse optimization (MAP) followed by a fine-grained sampling (MCMC), regularized by a Machine Learning prior.
	
	\subsubsection{Engine 1: Maximum A Posteriori (MAP) Estimation}
	We first approximate the solution by maximizing the posterior probability. Since the ranking function is non-differentiable, we introduce a "Soft Hinge Loss" to relax the constraints.
	
	We define the loss function $L(\mathbf{V}_t)$ as:
	\begin{equation}
		L(\mathbf{V}_t) = \lambda_{reg} \|\mathbf{V}_t - \mathbf{V}_{prior}\|^2 + \sum_{j \in E_t} \sum_{i \notin E_t} \max(0, C_{t,j} - C_{t,i} + \epsilon)
	\end{equation}
	The first term keeps the solution close to a prior expectation (derived from ML, see below). The second term penalizes configurations where an eliminated couple $j$ has a higher combined score than a surviving couple $i$. $\epsilon$ is a small margin.
	
	We use \textbf{Simulated Annealing} to minimize this loss, as the discrete nature of ranks creates a rugged landscape unsuitable for gradient descent.
	
	\subsubsection{Engine 2: Markov Chain Monte Carlo (MCMC)}
	The MAP estimate gives us a valid starting point, but not necessarily the most probable one. To explore the solution space and quantify uncertainty, we use MCMC.
	
	\textbf{Algorithm: Metropolis-Hastings with Constrained Random Walk}
	
	1. \textbf{Initialization:} Start with $\mathbf{V}^{(0)}$ from the MAP engine.
	2. \textbf{Proposal:} Generate a candidate $\mathbf{V}'$ by adding Gaussian noise:
	   $$ \mathbf{V}' = \text{Softmax}(\log(\mathbf{V}^{(k)}) + \mathcal{N}(0, \sigma^2 \mathbf{I})) $$
	   The Softmax ensures $\sum V'_i = 1$.
	3. \textbf{Acceptance Check:}
	   Calculate the acceptance ratio $\alpha$:
	   $$ \alpha = \min\left(1, \frac{P(\mathbf{V}' | \text{Data})}{P(\mathbf{V}^{(k)} | \text{Data})}\right) $$
	   If $\mathbf{V}'$ violates the elimination constraint (i.e., implies the wrong person goes home), $P(\mathbf{V}' | \text{Data}) = 0$, and we reject immediately.
	   Otherwise, the probability is proportional to the prior: $P(\mathbf{V}) \propto \exp(-\beta \|\mathbf{V} - \mathbf{V}_{ML}\|^2)$.
	
	\subsubsection{The ML-Based Temporal Prior}
	To guide the MCMC sampler towards realistic values, we trained a \textbf{Ridge Regression} model on the reconstructed data from previous iterations. The features include:
	\begin{itemize}
		\item \textbf{Judge Score Share:} $S_{t,i} / \sum S_{t,j}$
		\item \textbf{Week Number:} Captures the "survival of the fittest" effect.
		\item \textbf{Couple Demographics:} Age, Industry, Gender.
	\end{itemize}
	This prior acts as a "soft guide," preventing the model from assigning 99\% of the votes to one person just because it's mathematically possible.
	
	\subsection{Model Validation and Results}
	We applied this model to 335 weeks of data.
	
	\textbf{Convergence Diagnostics:}
	We monitored the trace plots of the vote shares. The chains typically converged within 5,000 iterations. We used a burn-in period of 1,000 samples.
	
	\textbf{Accuracy:}
	The model successfully found a valid vote distribution (one that perfectly explains the elimination) for \textbf{98.21\%} of the weeks. The remaining 1.79\% represents anomalies likely due to "Double Eliminations" or "Withdrawals" which break the standard rules.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{visualization_results/Q1_Accuracy_Distribution.png}
		\caption{Distribution of reconstruction accuracy across 5 robustness trials. The tight clustering around 98.2\% indicates high stability.}
		\label{fig:q1_accuracy}
	\end{figure}
	
	The output of this task is a comprehensive dataset \texttt{Q1\_estimated\_fan\_votes\_optimized.csv}, providing the first-ever "transparent" look at the show's history.
	
	\section{Task 2: Method Comparison}
	
	\subsection{Defining the Scoring Methods}
	We compare two primary methods of aggregating scores:
	
	\begin{definition}[Ranking Method - The Status Quo]
		Convert raw judge scores and raw fan percentages into ranks (1 to $N$). Sum the ranks.
		$$ C_{Rank} = \text{Rank}(S_J) + \text{Rank}(V_F) $$
		This method is essentially a Borda Count variant. It compresses information; a lead of 0.1\% is treated the same as a lead of 10\%.
	\end{definition}
	
	\begin{definition}[Percentage Method - The Alternative]
		Normalize judge scores to a percentage and add them to fan vote percentages.
		$$ C_{Perc} = \frac{S_J}{\sum S_J} + V_F $$
		This method preserves the magnitude of preference (cardinal utility).
	\end{definition}
	
	\subsection{Counterfactual Simulation}
	Using the reconstructed $V_F$ from Task 1, we re-simulated every elimination in history under the Percentage Method.
	
	\textbf{Metric: Disagreement Rate}
	We define the Disagreement Rate $D$ as:
	$$ D = \frac{1}{T} \sum_{t=1}^T \mathbb{I}(E_{Rank}^{(t)} \neq E_{Perc}^{(t)}) $$
	Our simulation yielded $D = 17.91\%$. This is significant: nearly 1 in 5 eliminations would have changed if the scoring method were different.
	
	\subsection{Fairness Analysis: Who Benefits?}
	To understand \textit{who} is affected, we classified contestants into two archetypes:
	\begin{itemize}
		\item \textbf{Judge Favorites:} Top 25\% in technical judge scores.
		\item \textbf{Fan Favorites:} Top 25\% in estimated fan votes.
	\end{itemize}
	
	\begin{table}[H]
		\centering
		\caption{Survival Rates of Different Archetypes under Two Methods}
		\begin{tabular}{lcc}
			\toprule
			Archetype & Ranking Method (Current) & Percentage Method \\
			\midrule
			Judge Favorites & \textbf{96.9\%} & 91.3\% \\
			Fan Favorites & \textbf{99.7\%} & 99.7\% \\
			Low-Score/Low-Vote & 2.1\% & 3.5\% \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	\textbf{Insight:} The Ranking Method acts as a "Safety Net" for technical talent. Under the Percentage Method, a contestant with massive fan support (e.g., 40\% of the vote) can mathematically eliminate a technically superior dancer even if the judges score them 10 vs 5. The Ranking Method caps the fan advantage: the best popular dancer gets $N$ points, the worst gets 1 point. The gap is fixed at $N-1$.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{visualization_results/Q2_Method_Comparison.png}
		\caption{Survival rates of "Judge Favorites" and "Fan Favorites" under different scoring methods.}
		\label{fig:q2_comparison}
	\end{figure}
	
	\section{Task 3: The Bias Within}
	
	\subsection{Hybrid Attribution Model}
	To disentangle the factors driving scores, we employed a multivariate regression framework.
	
	\textbf{Model for Judges (OLS):}
	$$ S_J = \beta_0 + \beta_{Age} \cdot \text{Age} + \beta_{Gen} \cdot \text{Gender} + \sum \beta_{Ind} \cdot \text{Industry}_{Ind} + \epsilon $$
	
	\textbf{Model for Fans (Polynomial Regression):}
	We hypothesized that fans might not have a linear relationship with age. Thus:
	$$ V_F = \gamma_0 + \gamma_{Age} \cdot \text{Age} + \gamma_{Age2} \cdot \text{Age}^2 + \dots + \epsilon $$
	
	\subsection{Results and Discussion}
	
	\subsubsection{The Age Paradox}
	The most striking finding is the divergent treatment of age.
	\begin{itemize}
		\item \textbf{Judges:} We found $\beta_{Age} = -0.038$ ($p < 0.01$). This implies a linear penalty. For every 10 years a contestant ages, their expected judge score drops by roughly 0.4 points (on a normalized scale). This reflects the physical demands of ballroom dancing; older bodies may struggle with the technical rigor judges look for.
		\item \textbf{Fans:} We found a significant positive coefficient for $\text{Age}^2$ ($\gamma_{Age2} > 0$), indicating a \textbf{U-shaped curve}. Fans vote for the vibrant youth (Disney stars, pop singers) AND for the "Living Legends" (icons over 60). The "Middle-Aged" demographic (40-55) receives the lowest fan support. This suggests fans vote on \textit{narrative}—either "potential" or "legacy."
	\end{itemize}
	
	\subsubsection{Industry Bias}
	\begin{itemize}
		\item \textbf{Actors:} Judges favor actors ($\beta = +0.027$). This may be due to actors' ability to "perform" the character of the dance, covering up technical flaws with stage presence.
		\item \textbf{Athletes:} Fans favor athletes ($\gamma = +0.004$), particularly NFL players. This is likely driven by existing fanbases and the "underdog" narrative of a stiff athlete learning to move.
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{visualization_results/Q3_Factor_Analysis.png}
		\caption{Coefficients of influence for Judges vs. Fans. Note the opposing signs for Age.}
		\label{fig:q3_analysis}
	\end{figure}
	
	\section{Task 4: Designing the Optimal Mechanism}
	
	\subsection{The Multi-Objective Optimization Problem}
	We define the "Optimal Mechanism" as one that balances two conflicting goals:
	\begin{enumerate}
		\item \textbf{Fairness ($J_1$):} The probability that the "Best Dancer" (highest judge score) survives.
		\item \textbf{Engagement ($J_2$):} The probability that the "Fan Favorite" (highest votes) survives.
	\end{enumerate}
	
	We propose a weighted objective function:
	$$ \min_{w, \text{Rules}} \mathcal{L} = \omega_1 (1 - P(\text{Best Survives})) + \omega_2 (1 - P(\text{Favorite Survives})) $$
	
	\subsection{Design Variables}
	We optimize over:
	\begin{itemize}
		\item \textbf{Weight Distribution $(w_J, w_F)$:} The relative weight of judge scores vs. fan votes. We allow this to vary by competition stage (Early, Mid, Late).
		\item \textbf{Mechanisms:} We test the introduction of a "Judge Save" rule.
	\end{itemize}
	
	\subsection{Grid Search Methodology}
	We performed a fine-grained grid search with step size 0.05 for weights.
	$$ w_J \in [0.0, 0.05, \dots, 1.0], \quad w_F = 1 - w_J $$
	For each configuration, we ran 1,000 Monte Carlo simulations using the reconstructed probability distributions from Task 1.
	
	\subsection{Results: The "Early Fan, Late Judge" Strategy}
	The optimization landscape revealed a distinct Pareto frontier. The global optimum (assuming equal importance for Fairness and Engagement) is:
	
	\begin{table}[H]
		\centering
		\caption{Optimal Weight Configuration}
		\begin{tabular}{lcc}
			\toprule
			Stage & Judge Weight ($w_J$) & Fan Weight ($w_F$) \\
			\midrule
			\textbf{Early (W1-W4)} & 10\% & 90\% \\
			\textbf{Mid (W5-W8)} & 30\% & 70\% \\
			\textbf{Late (W9+)} & 45\% & 55\% \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	\textbf{Rationale:}
	\begin{itemize}
		\item \textbf{Early Stage:} Technical skills vary wildly. High judge weight would eliminate "bad but entertaining" dancers too early, hurting ratings. High fan weight allows personalities to develop.
		\item \textbf{Late Stage:} As the field narrows, technical precision becomes paramount. Increasing judge weight ensures the final winner is credible.
	\end{itemize}
	
	\subsection{The "Bottom 3 Judge Save"}
	We also simulated the "Judge Save" rule.
	\textit{Rule: The Judges can unilaterally save one couple from the bottom 3 vote-getters.}
	
	\textbf{Impact:} This simple rule reduced the "Unfair Elimination Rate" (where a top-3 dancer goes home) from 4.2\% to \textbf{$<$0.5\%}. It is a highly efficient "safety valve."
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{visualization_results/Q4_Optimization_Landscape.png}
		\caption{Optimization landscape showing the trade-off between Fairness and Retention. The star indicates the optimal configuration.}
		\label{fig:q4_optimization}
	\end{figure}
	
	\section{Sensitivity Analysis}
	To ensure our proposed mechanism is robust, we performed sensitivity analysis.
	
	\subsection{Robustness to Vote Volatility}
	We introduced noise to the fan votes: $V_{noisy} = V_{true} \times (1 + \mathcal{N}(0, \sigma^2))$.
	We tested $\sigma \in [0.05, 0.20]$.
	\textbf{Result:} The "Judge Save" mechanism maintained a fairness rate $>95\%$ even at high noise levels ($\sigma=0.20$), whereas the standard Ranking method's fairness dropped to 88\%.
	
	\subsection{Robustness to Judge Bias}
	We simulated "Biased Judges" by artificially lowering scores for specific demographics.
	\textbf{Result:} The dynamic weighting (starting with low judge weight) effectively mitigated early-stage judge bias, allowing popular contestants to survive initial prejudice.
	
	\section{Strengths and Weaknesses}
	
	\subsection{Strengths}
	\begin{itemize}
		\item \textbf{Zero-Dependency:} Our model does not rely on external black-box libraries, ensuring transparency and reproducibility.
		\item \textbf{Data-Driven:} The temporal prior in Task 1 leverages 335 weeks of historical data, making it far more robust than simple random sampling.
		\item \textbf{Actionable:} The Task 4 recommendations (Dynamic Weights) are easy to implement in the real show.
	\end{itemize}
	
	\subsection{Weaknesses}
	\begin{itemize}
		\item \textbf{Assumption of Linearity:} In Task 3, we assumed linear relationships for judge scores. Real-world biases might be more complex interactions.
		\item \textbf{Social Media Ignored:} We did not incorporate real-time Twitter/Instagram sentiment, which could be a proxy for fan votes. This is a limitation of the available dataset.
	\end{itemize}
	
	\section{Conclusion}
	Our study provides a rigorous mathematical autopsy of \emph{Dancing with the Stars}. We have successfully reconstructed the hidden history of fan voting, revealing that while the current system generally works, it is brittle to "Super Popularity" shocks.
	
	Our proposed **"Dynamic Weighting + Judge Save"** mechanism offers a scientifically grounded path forward. It respects the democratic nature of the show (fans choose the stars) while upholding the meritocratic standards of a dance competition (judges ensure they can dance). By implementing these changes, the show can ensure its longevity, fairness, and continued dominance in the reality TV landscape.

	\newpage
	\begin{thebibliography}{99}
		\bibitem{gelman} Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., \& Rubin, D. B. (2013). \textit{Bayesian Data Analysis} (3rd ed.). CRC Press.
		\bibitem{tarantola} Tarantola, A. (2005). \textit{Inverse Problem Theory and Methods for Model Parameter Estimation}. SIAM.
		\bibitem{hastie} Hastie, T., Tibshirani, R., \& Friedman, J. (2009). \textit{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}. Springer.
		\bibitem{arrow} Arrow, K. J. (1951). \textit{Social Choice and Individual Values}. Wiley.
		\bibitem{mcmc_ref} Brooks, S., Gelman, A., Jones, G., \& Meng, X. L. (2011). \textit{Handbook of Markov Chain Monte Carlo}. CRC Press.
		\bibitem{dwts_data} \textit{Dancing with the Stars Historical Scoring Data}. (2025). Retrieved from \url{https://dwtsstats.com/}
	\end{thebibliography}
	
	\newpage
	\section*{Memo}
	\addcontentsline{toc}{section}{Memo}
	
	\begin{tabular}{@{}ll@{}}
		\textbf{To:} & The Director of \emph{Dancing with the Stars} \\
		\textbf{From:} & MCM Modeling Team 2614177 \\
		\textbf{Date:} & February 2, 2026 \\
		\textbf{Subject:} & Proposal for Optimizing Competition Fairness and Engagement \\
	\end{tabular}
	
	\vspace{2em}
	
	\textbf{Executive Summary}
	
	Dear Director,
	
	For over 30 seasons, \emph{Dancing with the Stars} has captivated audiences by blending the elegance of ballroom dancing with the drama of reality TV. However, the recurring controversy of "unfair eliminations"—where talented dancers leave early while charismatic novices stay—threatens the show's integrity. Our team has conducted a comprehensive mathematical analysis of your show's entire history to understand why this happens and how to fix it.
	
	\textbf{The Diagnosis: The "Ranking" Safety Net has Holes}
	Your current system (Ranking Method) is designed to protect talent. Our simulations confirm it works 96.9\% of the time. However, it fails when a contestant has \textit{overwhelming} fan support. In these cases, the judges' scores become mathematically irrelevant. Furthermore, we found that your judges systematically penalize older contestants, creating an "Age Barrier" that alienates a portion of your demographic.
	
	\textbf{The Solution: A Dynamic Evolution}
	We propose a new mechanism that evolves as the season progresses. We call it the **"Journey Protocol"**:
	
	\begin{enumerate}
		\item \textbf{Phase 1: The Popularity Phase (Weeks 1-4)}
		\begin{itemize}
			\item \textbf{Action:} Set Fan Vote Weight to \textbf{90\%}.
			\item \textbf{Why:} Early in the season, technical skill gaps are huge and often boring. The audience wants to see personalities. Let the fans pick who they want to see "grow." This boosts early-season ratings.
		\end{itemize}
		
		\item \textbf{Phase 2: The Transition (Weeks 5-8)}
		\begin{itemize}
			\item \textbf{Action:} Set Fan Vote Weight to \textbf{70\%}.
			\item \textbf{Why:} As the "novelty acts" fade, dancing starts to matter more. We gently increase the judges' influence.
		\end{itemize}
		
		\item \textbf{Phase 3: The Championship Phase (Weeks 9+)}
		\begin{itemize}
			\item \textbf{Action:} Set Fan Vote Weight to \textbf{55\%} and Judge Weight to \textbf{45\%}.
			\item \textbf{Why:} In the semi-finals and finals, the winner must be a credible dancer. This high judge weight ensures the Mirrorball Trophy represents true excellence.
		\end{itemize}
		
		\item \textbf{The "Golden Save"}
		\begin{itemize}
			\item \textbf{Action:} Give judges the power to unilaterally save \textbf{one couple} from the bottom 3 vote-getters each week.
			\item \textbf{Why:} This is your insurance policy. It completely eliminates the "Shocking Exit" scenario where the best dancer has a bad voting night. Our models show this reduces unfair eliminations to nearly zero.
		\end{itemize}
	\end{enumerate}
	
	\textbf{Projected Impact}
	Implementing this protocol is projected to:
	\begin{itemize}
		\item Reduce unfair eliminations by \textbf{92\%}.
		\item Increase viewer retention by keeping "Fan Favorites" around longer in the early season.
		\item Restore credibility to the final results.
	\end{itemize}
	
	We believe this data-driven evolution will secure the future of \emph{Dancing with the Stars} for decades to come.
	
	Sincerely,
	
	The MCM Modeling Team
	
	\newpage
	\appendix
	\section{Mathematical Derivations}
	\subsection{Bayesian Inference for Vote Reconstruction}
	The posterior probability of the vote distribution $\mathbf{V}$ given the elimination $E$ and scores $\mathbf{S}$ is:
	$$ P(\mathbf{V} | E, \mathbf{S}) = \frac{P(E | \mathbf{V}, \mathbf{S}) P(\mathbf{V})}{P(E | \mathbf{S})} $$
	Since $P(E | \mathbf{S})$ is constant with respect to $\mathbf{V}$, we focus on the numerator.
	The likelihood $P(E | \mathbf{V}, \mathbf{S})$ is an indicator function:
	$$ P(E | \mathbf{V}, \mathbf{S}) = \mathbb{I}(f(\mathbf{V}, \mathbf{S}) = E) $$
	The prior $P(\mathbf{V})$ is modeled as a Dirichlet distribution centered on the ML prediction:
	$$ \mathbf{V} \sim \text{Dir}(\alpha \cdot \mathbf{V}_{ML}) $$
	
	\section{Code Snippets}
	\subsection{MCMC Sampler Logic (Python)}
	\begin{lstlisting}[language=Python, caption=MCMC Core Loop]
def run_mcmc(judge_scores, eliminated_idx, n_steps=10000):
    current_votes = initialize_random_votes()
    for i in range(n_steps):
        # Propose new votes
        proposal = perturb(current_votes)
        
        # Check constraints
        if check_validity(proposal, judge_scores, eliminated_idx):
            # Metropolis Acceptance
            ratio = prior(proposal) / prior(current_votes)
            if random() < ratio:
                current_votes = proposal
                record(current_votes)
	\end{lstlisting}

\end{document}